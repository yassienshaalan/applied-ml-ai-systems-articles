# Uncertainty as a Conserved Quantity in Machine Learning Systems

This folder contains the Colab notebooks and supporting code for the article:

**‚ÄúUncertainty as a Conserved Quantity in Machine Learning Systems‚Äù**

The article argues that uncertainty in machine learning is not something that is
eliminated by optimisation, calibration, or confidence thresholds. Instead, it behaves
like a **system-level conserved quantity**: it is compressed, relocated, or deferred
across models, pipelines, and time.

Rather than treating uncertainty as a model defect, this work treats it as a **property
of the entire decision system**.

---

## üîó Article Link

Medium article:  
[Uncertainty as a Conserved Quantity in Machine Learning Systems](https://medium.com/@yassien/uncertainty-as-a-system-quantity-in-machine-learning-a985ffdc48dd)
 *link to be added once published*

---

## What This Article Is About

When ML systems become more confident, uncertainty has not disappeared.
It has been:

- Compressed into fewer decisions
- Pushed into different regions of the input space
- Deferred into future time steps
- Hidden behind thresholds, automation rules, or aggregation

This article explores that idea empirically using **small, controlled experiments**
designed to make uncertainty *visible*, rather than to optimise performance.

The goal is not to compare models, but to answer questions such as:

- Where does uncertainty concentrate when confidence increases?
- How do automation thresholds implicitly allocate uncertainty?
- What happens to uncertainty when decisions are deferred over time?

---

## Contents

