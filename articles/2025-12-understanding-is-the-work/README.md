# Understanding Is the Work: Why Machine Learning Cannot Be Reduced to Optimisation

This folder contains the Colab notebooks and supporting material for the article:

**‚ÄúUnderstanding Is the Work: Why Machine Learning Cannot Be Reduced to Optimisation‚Äù**

The article examines a growing assumption in modern ML practice:
that because optimisation can be automated, *understanding can be as well*.

This work argues the opposite.

Optimisation can improve metrics, tune hyperparameters, and scale workflows,
but **understanding remains a human responsibility**, especially in systems that
must be governed, explained, and held accountable.

---

## üîó Article Link

Medium article:  
**Understanding Is the Work: Why Machine Learning Cannot Be Reduced to Optimisation**  
[Understanding is the work: Why Machine Learning Cannot Be Reduced to Optimisation](https://medium.com/@yassien/understanding-is-the-work-why-machine-learning-cannot-be-reduced-to-optimisation-a12d9b78cef2)

---

##  What This Article Is About

As ML tooling becomes more powerful, it is increasingly tempting to treat modelling
as a procedural task:
- choose a model
- optimise a metric
- deploy the result

This article explores why that framing breaks down in practice.

In particular, it focuses on:
- The gap between strong metrics and genuine understanding
- How Goodhart‚Äôs Law manifests in ML systems
- Why the most dangerous failures come from *high-performing* models
- The distinction between procedural optimisation and interpretive judgement
- What changes when LLMs are introduced into the modelling loop

The goal is not to argue against automation or LLMs, but to clarify **where optimisation
ends and responsibility begins**.

---

## Contents

